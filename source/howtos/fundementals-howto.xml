<?xml version="1.0" encoding="UTF-8"?>
<article id="fundementals-howto" lang="tr">
  <articleinfo>
	  <title>Unix ve Internetin Temelleri NASIL</title>
	<author>
		<firstname>Eric Raymond</firstname>
		<affiliation><address><email>esr@thyrsus.com</email></address></affiliation>
	</author>
	<author role="translator">
        	<firstname>Cem</firstname><surname>Sönmez</surname>
      		<affiliation><address><email>cem@comu.edu.tr</email></address></affiliation>
	</author>
    <revhistory>
	    <para><emphasis role="bold">Bu çevirinin sürüm bilgileri:</emphasis></para>
	    <revision>
		    <revnumber>1.0</revnumber>
		    <date>Ekim 2008</date>
		    <authorinitials>cs</authorinitials>
		    <revremark>İlk sürüm</revremark>
	    </revision>
    </revhistory><revhistory>
	    <para><emphasis role="bold">Özgün belgenin sürüm bilgileri:</emphasis></para>
	    <revision>
		    <revnumber>2.10</revnumber>
		    <date>2007-11-28</date>
		    <authorinitials>esr</authorinitials>
		    <revremark>Küçük düzeltmeler</revremark> 
	    </revision>
    </revhistory>
    <legalnotice><title>Yasal Açıklamalar</title><para>
		    Bu belgenin, <emphasis>Unix ve Internetin Temelleri NASIL</emphasis> çevirisinin 1.0 sürümünün <emphasis role="bold">telif hakkı © 2008 <emphasis>Cem Sönmez</emphasis>'e</emphasis> aittir, özgün İngilizce sürümünün <emphasis role="bold">telif hakkı © 1999-2007 <emphasis>Eric Raymond</emphasis>'a </emphasis> aittir. Bu belgeyi, Free Software Foundation tarafından yayınlanmış bulunan <link linkend="gpl">GNU Genel Kamu Lisansının</link> 3 ya da daha sonraki sürümünün koşullarına bağlı kalarak kopyalayabilir, dağıtabilir ve/veya değiştirebilirsiniz. Bu Lisansın bir kopyasını <ulink url="http://www.gnu.org/licenses/gpl.html"/> adresinde bulabilirsiniz.
	    </para>
	    <para>
		    BU BELGE "ÜCRETSİZ" OLARAK RUHSATLANDIĞI İÇİN, İÇERDİĞİ BİLGİLER İÇİN İLGİLİ KANUNLARIN İZİN VERDİĞİ ÖLÇÜDE HERHANGİ BİR GARANTİ VERİLMEMEKTEDİR. AKSİ YAZILI OLARAK BELİRTİLMEDİĞİ MÜDDETÇE TELİF HAKKI SAHİPLERİ VE/VEYA BAŞKA ŞAHISLAR BELGEYİ "OLDUĞU GİBİ", AŞİKAR VEYA ZIMNEN, SATILABİLİRLİĞİ VEYA HERHANGİ BİR AMACA UYGUNLUĞU DA DAHİL OLMAK ÜZERE HİÇBİR GARANTİ VERMEKSİZİN DAĞITMAKTADIRLAR. BİLGİNİN KALİTESİ İLE İLGİLİ TÜM SORUNLAR SİZE AİTTİR. HERHANGİ BİR HATALI BİLGİDEN DOLAYI DOĞABİLECEK OLAN BÜTÜN SERVİS, TAMİR VEYA DÜZELTME MASRAFLARI SİZE AİTTİR.
	    </para>
	    <para>
		    İLGİLİ KANUNUN İCBAR ETTİĞİ DURUMLAR VEYA YAZILI ANLAŞMA HARİCİNDE HERHANGİ BİR ŞEKİLDE TELİF HAKKI SAHİBİ VEYA YUKARIDA İZİN VERİLDİĞİ ŞEKİLDE BELGEYİ DEĞİŞTİREN VEYA YENİDEN DAĞITAN HERHANGİ BİR KİŞİ, BİLGİNİN KULLANIMI VEYA KULLANILAMAMASI (VEYA VERİ KAYBI OLUŞMASI, VERİNİN YANLIŞ HALE GELMESİ, SİZİN VEYA ÜÇÜNCÜ ŞAHISLARIN ZARARA UĞRAMASI VEYA BİLGİLERİN BAŞKA BİLGİLERLE UYUMSUZ OLMASI) YÜZÜNDEN OLUŞAN GENEL, ÖZEL, DOĞRUDAN YA DA DOLAYLI HERHANGİ BİR ZARARDAN, BÖYLE BİR TAZMİNAT TALEBİ TELİF HAKKI SAHİBİ VEYA İLGİLİ KİŞİYE BİLDİRİLMİŞ OLSA DAHİ, SORUMLU DEĞİLDİR.
	    </para>
	    <para>
		    Tüm telif hakları aksi özellikle belirtilmediği sürece sahibine aittir. Belge içinde geçen herhangi bir terim, bir ticari isim ya da kuruma itibar kazandırma olarak algılanmamalıdır. Bir ürün ya da markanın kullanılmış olması ona onay verildiği anlamında görülmemelidir.
	    </para>
  </legalnotice>  
  <abstract>
	  <para>Bu belge PC-sınıfı bilgisayarların, Unix yürevi işletim sistemlerinin, ve Internet'in çalışma temellerini teknik olmayan dilde anlatıyor. </para><para>Bu belgenin İngilizce son sürümünü <ulink url="http://www.tldp.org/HOWTO/Unix-and-Internet-Fundamentals-HOWTO/index.html"/> adresinde bulabilirsiniz.</para>
    </abstract>
  </articleinfo>

  <sect1 id="fundementals-howto-intro"><title>Giriş</title>
	  <sect2 id="fundementals-howto-intro-purpose"><title>Amaç</title>
		  <para>Bu belge, yaparak öğrenmeye çalışan Unix ve Internet kullanıcılarına yardımcı olmak için tasarlanmıştır. Bu yöntem belli becerileri ortaya çıkarmak için çok iyi bir yöntem oluyorken, bazen kişinin bilgi temelinde tuhaf boşluklar bırakabiliyor - tuhaf boşluklar dediğimiz şey, yaratıcı düşünme ve etkin hata ayıklamayı güçleştiren faktördür.</para> 
		  <para>Tüm işlerin nasıl işlediğini basit ve sade bir şekilde anlatmaya çalışacağım. Sunum Unix ya da PC-sınıfı makinelerdeki Linux kullanıcılarına göre ayarlanacaktır. Bununla beraber, genellikle Unix' e değineceğim, sıklıkla anlatacağım sabit olan şeyleri ise farklı makinelere ve Unix türevlerine göre anlatacağım. </para>
		  <para>Intel PC kullandığınızı varsayıyorum. Eğer PowerPC ya da başka bir çeşit bilgisayar kullanıyorsanız, ayrıntılar ufak farklılıklar göterebilir.</para>
		  <para>Söylemiş olduğum şeyleri tekrarlamayacağım, bu yüzden dikkatinizi vermelisiniz, ancak bu demek değildir ki okuduğunuz her kelimeyi öğreneceksiniz. İlk defa okuduğunuz şeyleri üstünkörü bir şekilde okumanız iyi fikir; öğrendiğiniz şeyleri kavradığınız zaman birkaç kere geriye dönüp tekrar okuyabilirsiniz.</para>
		  <para>Bu sürekli gelişen bir belgedir. Bölümleri kullanıcının geribildirimlerine yanıt olarak ekliyorum, bu yüzden periyodik olarak geriye dönüp tekrar gözden geçirmelisiniz.</para>
	</sect2>
	<sect2 id="fundementals-howto-intro-new"><title>Belgenin yeni sürümleri</title>
		<para>Unix ve Internetin Temelleri NASIL'ın yeni sürümleri periyodik olarak comp.os.linux.help,comp.os.linux.announce ve news.answers'a postalanır. Bu sürümler Linux Documentation Project anasayfasında da belirtilen çeşitli websitelerine yüklenir. </para>			
		<para>Son sürümünü http://www.tldp.org/HOWTO/Unix-and-Internet-Fundamentals-HOWTO/index.html URL vasıtasıyla görüntüleyebilirsiniz.</para>		
		<para>Bu belge Lehçe(Polca), İspanyolca ve Türkçe dillerine çevrilmiştir.</para>
	</sect2>	
	<sect2 id="fundementals-howto-intro-feedback"><title>Geribildirim ve düzeltmeler</title>
		<para>Eğer bu belge ile ilgili herhangi bir sorunuz veya yorumunuz varsa, lütfen Eric S. Raymond'a  esr@thyrsus.com'dan mail atmaktan çekinmeyiniz. Her türlü öneri ve eleştirilerinize açığımdır. Özellikle her kavramın detaylı açıklanmış hiperbağlaçlara ilgili olanlara açığımdır. Eğer bu belgede bir hata bulursanız, lütfen bana bildirin, böylelikle ben de bir sonraki sürümde yanlışları düzeltebileyim. Teşekkürler.</para>
	</sect2>
	<sect2 id="fundementals-howto-intro-related"><title>Başvurulan kaynaklar</title>
		<para>Eğer bu belgeyi nasıl hackleme yapabileceğini öğrenmek için okuyorsanız, Nasıl Hacker Olunur SSS'i de okumalısınız. Bu belge bazı diğer kullanışlı kaynakların bağlantılanı içeriyor.</para>
	</sect2>
</sect1>

<sect1 id="fundementals-howto-basic-anatomy"><title>Bilgisayarınızın temel anatomisi</title>
	<para>Bilgisayarınızın içinde asıl işi yapan bir işlemci çipi bulunur. Bu çipin dahili belleği bulunur (DOS/Windows camiası "RAM", Unix camiası ise çoğu kez "çekirdek" olarak çağırır; Unix terminolojisinde bellek demir parçalarından oluşan RAM'dir. Bilgisayarınızın kalbi olan işlemci ve bellek anakartta bulunur.</para>
	
	<para>Bilgisayarınızın bir ekranı ve klavyesi vardır. Sabit diski ve bir CD-ROM'u veya bir floppy disk'i vardır. Bu cihazlardan bazıları anakarta yerleştirilmis denetleyici kartlar ile çalışır; diğer cihazlar ise, direk olarak anakartta bulunan, denetleyici kartlar ile aynı fonksiyonlara sahip, özelleşmiş çipsetler aracılığı ile çalışırlar. Klavyeniz ayrı bir karta ihitiyaç duyacak kadar basittir; denetleyici ise klavyedeki şasesine gömülüdür.</para>
	
	<para>Bu cihazların nasıl çalıştığı konusundaki detaylara daha sonra gireceğiz. Şimdilik, birlikte nasıl çalıştıkları konusunda ,aklımızda kalması için temel birkaç şey :</para>
	
	<para>Bilgisayarınızın kasası içindeki her parça bir veriyolu ile bağlıdır. Fiziksel olarak veriyolu, denetleyici kartlarınızın takılı olduğu şeydir (görüntü kartı, disk denetleyici, varsa ses kartı). Veriyolu, işlemciniz, ekranınız, diskiniz ve diğer herşeyiniz arasındaki veri anayoludur.</para>
	
	<para>(PC'lerle bağlantıda 'ISA', 'PCI' ve 'PCMIA' kaynaklarını gördüyseniz ve anlamadıysanız, bunlar veriyolu tipleridir. ISA ufak detaylardan hariçtir, aynı veriyolu IBM' in 1980 deki orjinal PC'lerinde kullanılmıştır; şimdilerde kullanım dışıdır. PCI, Çevresel Bileşen Bağlantısı, çoğu modern PC'lerde ve Macintosh'larda kullanılan veriyoludur. PCMIA, dizüstü bilgisayarlarda kullanılan, daha ufak fiziksel konnektörleri olan ISA'nın değişik bir biçimidir.) </para>
		
	<para>Herşeyi yürüten işlemci aslında diğer parçaları direk olarak göremez; bu parçalarla veriyolu hattı aracılığı ile haberleşmek zorundadır. Sadece diğer altsistem, belleğe (çekirdek) hızlı ve doğrudan erişime sahiptir. Programların sırayla çalışması için,  çekirdek (bellek) içinde bulunmalıdırlar.</para>
	
	<para>Bilgisayarınız diskten bir program ya da veri okuduğu zaman, aslında işlemci disk okuma isteğini disk denetleyicisine gönderirken veriyolunu kullanıyordur. Biraz zaman sonra disk denetleyicisi, veriyi okuma ve belleğin belirli bir yerine koyma için işlemciye sinyal göndermek için veriyolunu kullanır. İşlemci daha sonra bu veriye bakmak için veriyolunu kullanabilir.</para>
		
	<para>Klavyeniz ve ekranınız da işlemci ile veriyolu aracılığı ile haberleşir, ancak daha basit yollardan. Bunralı daha sonra tartışacağız. Şimdilik, bilgisayarınızı açtığınız zaman neler olduğunu anlamanız yeterli olacaktır.</para>	
</sect1>

<sect1 id="fundementals-howto-switch-on"><title>Bilgisayarı açtığınızda neler olur ?</title>
	<para>Çalışan bir program olmadığında, bir bilgisayar sadece elektroniğin hareketsiz, iri bir parçasıdır. İlk olarak bir bilgisayar açıldığı zaman başlangıçta, işletim sistemi denen özel bir programı çalıştırmak zorundadır. İşletim sisteminin görevi; diğer bilgisayar programlarının çalışmasına, bilgisayar donanımının kontrolündeki düzensizlikleri ele alarak yardımcı olmaktır.</para>
	
	<para>İşletim sistemini getiren süreç, önyükleme (boot etme) olarak adlandırılır (***aslında bu ***). Bilgisayarınız nasıl önyükleme yapacağını bilir, çünkü önyükleme için gerekli ifadeler bir tane çipe yerleştirilmiştir, BIOS (Basic Input/Output System - Temel Giriş/Çıkış Sitemi) çipi.</para>
	
	<para>BIOS çipi bilgisayarınıza, önyükleme yükleyicisi (Linux'ta önyükleme yükleyicisi Grub ya da LILO olarak çağrılır) denilen özel bir program için, genellikle en düşük numaralı sabit disk (önyükleme diski) olan belirli bir yer bakmasını söyler. Önyükleme yükleyicisi belleğe çekilir ve başlatılır. Önyükleme yükleyicisinin işi; gerçek işletim sistemini başlatmaktır.</para>	
			
	<para>Yükleyici bu işi bir çekirdek aramak, onu belleğe yüklemek ve başlatmak için yapar. Linux'u başlattığınız ve LILO ekranını izleyen bir nokta demeti gördüğünüz zaman, çekirdek yükleniyordur. (Her nokta ,çekirdek kodunun farklı bir disk bloğunun yüklendiğini gösterir.)</para>
	
	<para>(Neden BIOS'un çekirdeği direk olarak yüklemediği ve önyükleme yükleyicisi ile iki adımlık bir sürec olduğuna şaşırabilirsiniz. BIOS pek de akıllı değildir. Aslında çok aptaldır ve Linux onu önyükleme zamanından sonra bir daha kullanmaz. Başlangıçta BIOS, 8 bitlik ilkel PC'lerin ufak diskleri için yazılmıştı ve çekirdeği direk olarak yüklemek için diske yeteri kadar erişemiyordu. Önyükleme yükleyici adımı, diskinizin farklı bölümlerindeki diğer işletim sistemlerini de başlatma imkanı verir. Kötü olan olay ise; bu konuda Unix pek iyi değildir.)</para>
	
	<para>Bir kere çekirdek başladığı zaman etrafına bakar, uyuyan donanımları bulur ve programları çalıştırmaya hazır hale getirir. Bu işi ise sıradan bellek yerlerini uyarmaktan ziyade, giriş/çıkış portları ile yapar - uygun özel veriyolu adresleri, komutlar için bu portları dinleyen denetleyici kartlarına sahiptir. Çekirdek rastgele uyarı yapmaz; denetleyicilerin hazır olanları nerede bulacağı ve nasıl yanıt vereceği konusunda pek çok bilgiye sahiptir. Bu işleme ise ***autoprobing*** denir.</para>
	
	<para>Önyükleme zamanıda mesajların çoğunu, çekirdeğinizin girişi/çıkış portları etrafındaki donanımı ***autoprobe etmesi, ...  *** Linux çekirdeği bu konuda, pek çok diğer Unix'lerden ve DOS veya Windows'tan çok daha iyidir. Aslında, birçok tecrübeli Linux kullanıcıları,*** Linux'un önyükleme zamanı incelemesinin becerisi (yükleme işini oldukça kolay yaptığını) hakkında düşünürler. ***</para>
	
	<para>Ancak çekirdeğin tamamen yüklenmesi ve çalışması önyükleme işleminin sonu değildir; bu sadece ilk bölümdür (bazen 1. seviye çalışması denir). Bu ilk bölümden sonra çekirdek, 'init' denilen, ev idaresi süreçleri yumurtlayan özel bir sürecin kontrolünü ele alır.</para>
	
	<para>İnit sürecinin ilk işi; genellikle disklerinizi tamam olduğundan emin olmak için denetlemektir. Disk dosya sistemleri narin şeylerdir; eğer bir donanım hatası veya elektrik kesintisi nedeniyle hasar görürse, Unix'inizin gidişatından önce kurtarma adımlarını uygulamanız için yeterli nedenleriniz var demektir. Bu konuya daha sonra, dosya sistemlerinde işlerin nasıl yanlış gittiği konusunda konuştuğumuz zaman gireceğiz.</para>
	
	<para>İnitin sonraki işi ise birkaç artalan sürecini çalıştırmaktır. Artalan süreci, arkaplanda gizlenip birşeylerin olması bekleyen yazıcı kuyruklayıcısı, posta dinleyicisi ya da WWW sunucusu gibi bir programdır. Bu özel programlar, sıklıkla birkaç isteğin çakışmasını düzenlemekle yükümlüdürler. ***Onlar artalan sürecleridir, çünkü sabit olarak çalışan ve bir kopya sürüsünün () birbirlerine girmediğinden emin olmak*** </para>
	
	<para>Bir sonraki adım ise kullanıcılara hazırlanmaktır. İnit konsolunuzu izlemek için, 'getty' denilen bir programın kopyasını başlatır (ve dial-in seri portlarını izlemek için belki daha fazla kopya). Bu program konsola giriş isteğinizi dağıtır. Bir kere her terminal için tüm artalan süreçleri ve getty süreçleri başladığında, 2. seviyede çalışıyoruz demektir. Bu seviyede, giriş yapabilir ve programları çalıştırabilirsiniz.</para>

	<para>Ancak henüz bitmedi. Bir sonraki adım; ağ işlemlerini ve diğer servisleri destekleyen çeşitli artalan süreçlerini çalıştırmaktır. Bir kere bu işlem yapıdığında, 3. seviyede çalışıyoruz demektir ve sistem tam olarak kullanıma hazırdır.</para>
</sect1>

<sect1 id="fundementals-howto-login"><title>Giriş yaptığınız zaman ne olur ?</title>
	
	<para>Giriş yaptığınız zaman (getty'e bir isim veriyoruz) bilgisayara kendinizi tanıtmış olursunuz. Daha sonra, sizin parolanızı alan ve makineyi kullanmak için yetkili kişi olup olmadığınızı kontrol eden login adında bir program çalıştırılır. Eğer değilseniz, giriş isteğiniz reddedilecektir. Eğer yetkili kişi iseniz, login ev idaresi ile ilgili birkaç şey yapar ve bir komut yorumlayıcısını çalıştırır, kabuğu. (Evet, getty ve login bir program olabilirdi. Bu programlar, burada olan kıymetsiz tarihsel nedenlerden dolayı ayrılmış durumdadır.)</para>
	
	<para>Burada, sistem size kabuğu vermeden önce neler yaptığı konusunda daha çok şey vardır (bu konuya dosya izinleri konusunda konuştuğumuz zaman ihtiyaç duyabilirsiniz). Kendinizi bir login adı ve parolası ile tanıtırsınız. Bu login adı, her satırı bir kullanıcı hesabını tanımlayan bir satır dizisi olan, /etc/passwd olarak adlandırılan dosyanın içerisinde aranır.</para>	
	
	<para>Bu alanlardan biri, hesap parolasının şifrelenmiş halidir (kimi zaman şifrelenmiş alanlar, aslında daha kısıtlı izinlerle /etc/shadow dosyasının içinde tutulur; bu durum parolanın kırılmasını zorlaştırır). Hesap parolası olarak girmiş olduğunuz şey tamamen aynı yolla şifrelenir ve login programı uyumluluğunu kontrol eder. Bu metodun güvenliği şu gerçeğe bağlıdır: parolanızın sade halinden şifrelenmiş haline geçiş kolay oluyorken, tersi durum çok zordur. Böylece, parolanızın şifrelenmiş halini görseler bile, hesabınızı kullanamazlar. (Bu demek oluyor ki, parolanızı unuttuğunuz taktirde, geri almanızın bir yolu yoktur, sadece sizin seçtiğiniz şeylere bağlı olarak değiştirme imkanınız vardır.)</para>	
	
	<para>Bir kere başarılı bir giriş yaptığınızda, kullanıyor olduğunuz kişisel hesabınızla ilişkilendirilmiş hakların tümüne sahip olursunuz. Bir grup, sistem yöneticisi tarafından kurulan, kullanıcıların toplamına verilen addır. Grupların, üyelerinin haklarından bağımsız olarak kendi hakları olabilir. (Unix haklarının nasıl çalıştığı ile ilgili detaylar için, aşağıdaki haklar ile ilgili bölümüe göz atabilirsiniz.)</para>
	
	<para>(Şunu not edelim: kullanıcılara ve gruplara ismiyle hitap edecek olsanız da, aslında sayısal ID'ler olarak tutulurlar. Otomatik olarak hesaplar ve gruplar arasındaki dönüşümle ilgilenen komutlardır.)</para>
	
	<para>Your account entry also contains your home directory, the place in the Unix file system where your personal files will live. Finally, your account entry also sets your shell, the command interpreter that login will start up to accept your commmands.</para>
	
	<para>Hesap girdiniz, Unix dosya sisteminde kişisel dosyalarınızın bulunacağı yer olan ev dizinizi de içerir. Son olarak hesap girdiniz, login'in çalıştıracağı, komutlarınızı kabul edecek komut yorumlayıcısı olan kabuğunuzu da ayarlar.</para>
</sect1>

<sect1 id="fundementals-howto-run-programs"><title>What happens when you run programs from the shell?</title>
	
	<para>Kabuk, girmiş olduğunuz komutlar için Unix'in yorumlayıcısıdır; kabuk diye çağırılmasının sebebi işletim sistemi çekirdeğini sarıyor ve gizliyor olmasıdır. Unix'in önemli bir özelliği; kabuğun ve çekirdeğin bir seri sistem çağrılarıyla iletişiminde ayrı programlar olmasıdır. Bu durum, arayüzlerdeki farklı tatlara uygun çoklu kabuğu mümkün kılıyor.</para>
	
	<para>Giriş yaptıktan sonra, normal kabuğun vermiş olduğu'$' bilgi ekranını görürsünüz (başka birşey olacak şekilde özelleştirmediyseniz). Ekranda görebileceğiniz kabuk sözdizimi ve kolay şeylerden bahsetmeyeceğiz; onun yerine bilgisayarınızdaki görüntünün arkasındaki kısım olan sahne arkasına bakacağız.</para>
		
	<para>Önyükleme anından sonra ve bir program çalıştırmadan önce bilgisayarınızı, bir hayvanat bahçesi kadar, birşeyler yapmak için bekleyen süreci içeren birşey olarak düşünebilirsiniz. Bunların tümü olaylara hizmet vermek için beklerler. Bir olay bir tuşa basmanız veya fareyi hareket ettirmeniz olabilir. Ya da eğer makineniz bir ağa takılmış durumda ise, ağ üzerinden bir veri paketinin geldiği bir olay olabilir.</para>

	<para>Çekirdek ise bu süreçlerden biridir. Özel bir tanesidir, çünkü diğer süreçler çalışabildiğinde o kontrol eder ve genelde makinenin donanımına direk erişebilen tek süreç çekirdektir. Aslında, kullanıcı süreçleri klavye girişi almak, ekranınıza yazmak, diskten okuma ve yazma yapmak ya da bellekteki bitleri çatır çatır çiğnemek dışında herhangi birşey yapmak istediğinde çekirdekten istekte bulunmak zorundadır. Bu istekler sistem çağrıları olarak bilinirler.</para>
	
	<para>Genelde, tüm giriş/çıkış'lar çekirdek üzerinden yürür, böylelikle çekirdek işlemleri programlayabilir ve süreçlerin birbirinin üzerine çıkmasına engel olur. Genellikle giriş/çıkış portların direk erişiminin verildiği birkaç özel sürecin çekirdeği es geçmesine izin verilmiştir. X sunucuları (çoğu Unix makinelerinde ekran grafiklerini oluşturmak için öteki program isteklerini idare eden programlardır) bunun en genel örneğidir. Ancak henüz bir komut satırımız yok; şu an komut satırına bakıyor durumdasınız.</para>	
	
	<para>Kabuk yalnızca bir kullanıcı sürcidir ve bilhassa özel bir tanesi değildir. Klavye giriş/çıkış portlarını dinleyerek (çekirdek vasıtasıyla) tuş darbelerini bekler. Çekidek bunları görecek olursa, ekranınıza yansıtır. Eğer çekirdek bir 'Enter' gördüğü zaman, metninizin satırını kabuğa geçirir. Kabuk da bu tuş darbelerini komut olarak yorumlamaya çalışır.</para>
	
	<para>Unix dizin listeleyicisini çağırmak için 'ls' yazın ve 'Enter'a basın. Kabuk, /bin/ls dosyası içerisindeki çalıştırılabilir dosyayı yürütmek için yerleşik kurallara başvurur. /bin/ls'yi bir alt süreç olarak başlatmak ve çekirdek vasıtasıyla ekrana ve klavyeye erişim izni vermek için çekirdeğe bir sistem çağrısı yapar. Daha sonra kabuk, ls'nin bitmesi için beklerken uykuya geçer.</para>
	
	<para>/bin/ls yapıldığı zaman, çekirdeğe bir sistem çağrısı yayarak bittiğini söyler. Daha sonra çekirdek kabuğu uyandırır ve çalışmaya devam edebileceğini söyler. Kabuk başka bir bilgi sistemi yayınlar ve başka bir giriş satırı için bekler.</para>
	
	<para>'ls' çalıştığı sırada başka şeyler çalışıyor olmalı, ancak(çok uzun bir dizin listelediğinizi farzetmek zorundayız). Başka bir sanal konsol açabilir, giriş yapabilir ve bir Quake oyununa başlayabilirsiniz örneğin. Ya da, Internet'e takıldığınızı farzedin. /bin/ls çalıştığı sırada, makineniz posta gönderiyor veya alıyor olabilir.</para>
</sect1>

<sect1 id="fundementals-howto-input-devices"><title>Giriş cihazları ve kesmeler nasıl çalışır?</title>
	<para>Klavyeniz çok basit bir giriş cihazıdır; basittir çünkü az miktarda veriyi çok yavaş bir şekilde üretir(bilgisayar standartları ile). Bir tuşa bastığınızda ya da bıraktığınızda bu durum, klavye kablosuna bir donanım kesmesi için sinyal gönderir.</para>	
	
	<para>Bu tarz kesmeleri izleme işi de işletim sisteminin görevidir. Olası her bir kesme çeşidi için, kesmelerle beraber verileri (tuşa basma/bırakma değerleri gibi) çalıştırılıncaya dek saklayan bir kesme işleyicisi olacaktır.</para>
	
	<para>Aslında kesme işleyicisi klavyeniz için tuş değerlerini belleğin yanında bulunan bir sistem alanına gönderir. Orada ise, işletim sisteminin klavyeden okuma yapması beklenen programa kontrolü verdiği zaman, inceleme için kullanılabilir hale gelecektir.</para>		
	
	<para>Disk veya ağ kartı gibi pek çok karmaşık giriş cihazları benzer yollarla çalışırlar. Daha önce, yerine getirilmiş olan bir disk istek sinyalini göndermek için kullanan bir disk denetleyicisinden bahsetmiştim. Aslında disk bir kesme çıkardığında ne olur. Disk kesme denetleyicisi, isteği yapan programın daha sonraki kullanımı için tekrar getirilen veriyi belleğe kopyalar.</para>

	<para>Her bir çeşit kesmenin kendisi ile ilişkilendirilmiş bir öncelik seviyesi vardır. Düşük seviyeli kesmeler (klavye durumları gibi) yüksek öncelikli kesmeleri (saat darbeleri veya disk durumları) beklemek zorundadır. Unix, makinenin yanıt düzenini korumak için, hızlıca işlenmesi gereken durumlara yüksek öncelik verecek şekilde tasarlanmışır.</para>

	<para>İşletim sisteminizin önyükleme zamanı mesajlarında, IRQ numaralarını görüyor olmalısınız. Neden olduğunu anlamadan şundan haberdar olun; donanımın yapılandırılamamasının en genel yolu, iki farklı cihazın aynı IRQ' yu kullanmaya çalışmasıdır.</para>
		
	<para>Yanıtı burada. IRQ (Interrupt Request - Kesme İsteği) nin kısaltmasıdır. İşletim sistemi, açılması esnasında hangi numaralı kesmelerin her bir donanımını kullanacak olduğunu bilmeye ihtiyacı vardır. Böylece her biri ile uygun olan işleyiciyi ilişkilendirir. Eğer iki farklı cihaz aynı IRQ'yu kullanmaya çalışırsa, kimi zaman kesmeler yanlış işleyiciye gönderilir. Genellikle bu durum cihazın kilitlenmesine ve işletim sisteminin uyku moduna geçmesine ya da çökmesine neden olacak şekilde işletim sistemini kötü bir biçimde yanıltır.</para>	
</sect1>

<sect1 id="fundementals-howto-timesharing"><title>Bilgisayarım birkaç işi tek seferde nasıl yapar?</title>
	<para>Aslında yapmaz. Bilgisayarlar bir zaman diliminde yalnızca bir görevi (ya da süreci) yapabilir. Ancak bir bilgisayar görevleri çok hızlı bir şekilde değiştirebilir ve ağır olan insan duyularını, birkaç işi sanki tek bir seferde yapıyormuş gibi düşündürerek kandırır. Bu işleme "zaman paylaşımı-timesharing" denir.</para>

	<para>Çekirdeğin işlerinden birisi, zaman paylaşımını yönetmektir. Çekirdeğin, planlayıcı adında, hayvanat bahçenizdeki diğer tüm (çekirdek olmayan) süreçleri hakkındaki bilgileri kendi içinde tutan bir parçası vardır. Her 1/60 saniyede, bir zamanlayıcı çekirdekteki işini bir saat kesmesi üreterek tamamlar. Planlayıcı, o anda çalışan süreçlerin hepsini durdurur, askıya alır ve başka bir sürecin kontrolüne el verir.</para>

	<para>1/60 saniye uzun bir zaman gibi görünmeyebilir. Ancak günümüz mikroişlemcileri, çok miktarda iş yapan on binlerce makine komutunu çalıştırabilirler. Çok fazla süreciniz olsa bile, her biri kendi zaman diliminde bir miktar işi tamamlayabilirler.</para>

	<para>Pratikte, bir program zaman diliminin tamamını almamalıdır. Eğer bir giriş/çıkış cihazından bir kesme gelirse, çekirdek etkin olarak mevcut görevi durdurur, kesme işleyicisini çalıştırır ve daha sonra mevcut göreve geri döner. Bir yüksek öncelikli kesme fırtınası normal süreci tamamen çıkarabilir; bu yanlış davranış 'dayak' olarak adlandırılır. İyi ki modern Unixlerde bu işlemin gerçekleşmesi oldukça zordur.</para>

	<para>Aslında, programların hızları, alabilecekleri bir miktar makine zamanı (bu kuralda birkaç istisnai durum vardır, örneğin ses veya üç boyutlu grafik nesli) tarafından nadiren limitlenir. Çok daha fazla sıklıkla, gecikmeler program disk sürücüsünden veya ağ bağlantısından veri beklemek zorunda olduğu zaman meydana gelir.</para>
	
	<para>Bir işletim sistemi, "Çok görevlilik" olarak adlandırılan çok sayıda eşzamanlı süreçleri düzenli olarak destekleyebilir. İşletim  sistemlerinin Unix ailesi baştan başa çok görevlilik için tasarlanmıştır ve bu işte oldukça iyidir - daha sonradan düşünülerek eklenilen ve oldukça başarısız bir biçimde yapılan Windows'lardan ya da eski Mac OS'lardan çok daha etkindir. Etkin, güvenilir çok görevlililiğin çok büyük bir kısmı, ağda, iletişimde ve web servisinde Linux'u üstün kılar.</para>	
</sect1>

<sect1 id="fundementals-howto-memory-management"><title>Bilgisayarım süreçlerinin birbiri üzerine çıkmasına nasıl engel olur?</title>
	<para>Çekirdek planlayıcısı, süreçlerin bir zamanda dilimine bölünmesi işine bakar. işletim sisteminiz de bu bu süreçleri bir alana bölmek zorundadır, böylece süreçler birbirinin üzerine çıkamazlar, bellek çalışması. Tüm programların işbirliği içinde olmaya çalıştığını varsaysanız bile, içlerinden birindeki, diğerlerin işini bozabilecek bir hatanın olmasını istemezsiniz. İşletim sisteminizin bu problemi çözmek için yaptığı işe bellek yönetimi denir.</para>

	<para>Hayvanat bahçenizdeki her süreç, kodu çalıştıracağı ve değişkenleri ve sonuçları içinde tutacağı kendi bellek alanına ihtiyaç duyar. Bu takımın salt okunur bir kod parçasından (süreç komutlarını içeren) ve yazılabilir veri parçalarından (süreç değişken belleğinin tümünü içeren) oluştuğunu düşünebilirsiniz. Veri parçası her süreç için gerçekten tektir. Ancak aynı kodu iki süreç çalıştırırsa, Unix otomatik olarak bu süreçleri yalnız bir kod parçasını paylaşacak şekilde verimli bir ölçüde düzenler.</para>
	
	
	
	<sect2 id="fundementals-howto-memory-management-simple"><title>Sanal bellek: basit sürüm</title>
		<para>Verimlilik önemlidir, çünkü bellek pahalıdır. Kimi zaman tüm programların tamamını yeterince tutmak zorunda olmayabilirsiniz, özellikle X sunucusu gibi büyük bir programı kullanıyorsanız. Bundan kaçınmak için  Unix, sanal bellek denilen bir tekniği kullanır. Sanal bellek, bellekteki bir süreç için kodun ve verinin tamamını tutmaya çalışmaz. Onun yerine, yalnızca nispeten daha kısa bir çalışma takımını bulundurur; işlemcinin rahat olması durumu ise diskinizdeki özel bir takas alanında saklıdır.</para>		

		<para>Dikkat edin, bir önceki paragraftaki "kimi zaman", "hemen hemen her zaman" anlamında. Genellikle bellek alanınızın boyutu çalışan programların boyutlarıyla az miktarda ilişkilidir, bu sebeple takaslama yaygındır. Bugünlerde bellek daha az pahalıdır ve alt uç makineler bile pek çoğuna sahiptir. 64 MB'lık tek kullanıcılı makinelerde ve sonrakilerde, X'i ve tipik bir görev karışımı başlangıçta çekirdeğe yüklendikten sonra, takaslama olmadan çalıştırmak mümkündür.</para>	
	</sect2>
	<sect2 id="fundementals-howto-memory-management-detailed"><title>Sanal bellek: detaylı sürüm</title>
		<para>Aslında, son kısım birşeyleri biraz olsun basitleştirmiştir. Evet, programlar belleğinizi fiziksel bellekten daha büyük, yassı bir adres bankası ve disk takaslamayı da bu ilüzyonun bakımında kullanılıyor olarak görür. Ancak donanımınız aslında, içerisinde beş farklı tür bellekten daha azına bulundurmaz ve programların maksimum hız için ayarlanması zorunluluğu olduğu zaman aralarındaki farklılıklar iyi bir anlaşma konusu olabilir. Gerçekten makinenizde neler olup bittiğini anlamak için, nasıl çalıştıklarını anlamalısınız.</para>
		
		<para>Beş bellek türü şunlardır : işlemci yazmaçları, dahili (veya çip üstü) önbellek, harici (veya çip dışı) önbellek, ana bellek ve disk. Çok tür olmasının nedeni ise basittir: hız para demektir. Bellek türlerini erişim zamanına göre artan, maliyete göre ise azalan sırada listeledim. Disk en yavaş, en ucuz ve saniyede 100 rastgele erişim yapabiliyor iken, yazmaç belleği en hızlı, en pahalı ve saniyede milyarlarca kez rastgele erişilebilir bir bellektir.</para>

		<para>Burada, tipik bir masaüstü makinesi için ilk 2000 hızı yansıtan bir listenin tamamı vardır. Hız ve kapasite artıp, fiyatlar düşüyor iken, bu oranların adil bir şekilde sabit olarak kalacağını ve o oranların bellek hiyerarşisini biçimlendirdiğini düşünebilirsiniz.</para>
		
		<para>Disk
			
			Size: 13000MB Accesses: 100KB/sec</para>
		
		<para>Ana Bellek
			
			Size: 256MB Accesses: 100M/sec</para>
		
		<para>Harici Önbellek
			
			Size: 512KB Accesses: 250M/sec</para>
		
		<para>Dahili Cache
			
			Size: 32KB Accesses: 500M/sec</para>
		
		<para>İşlemci
			
			Size: 28 bytes Accesses: 1000M/sec</para>
			
		<para>We can't build everything out of the fastest kinds of memory. It would be way too expensive — and even if it weren't, fast memory is volatile. That is, it loses its marbles when the power goes off. Thus, computers have to have hard disks or other kinds of non-volatile storage that retains data when the power goes off. And there's a huge mismatch between the speed of processors and the speed of disks. The middle three levels of the memory hierarchy (internal cache, external cache, and main memory) basically exist to bridge that gap.</para>
			
		<para>Linux and other Unixes have a feature called virtual memory. What this means is that the operating system behaves as though it has much more main memory than it actually does. Your actual physical main memory behaves like a set of windows or caches on a much larger "virtual" memory space, most of which at any given time is actually stored on disk in a special zone called the swap area. Out of sight of user programs, the OS is moving blocks of data (called "pages") between memory and disk to maintain this illusion. The end result is that your virtual memory is much larger but not too much slower than real memory.</para>
			
		<para>How much slower virtual memory is than physical depends on how well the operating system's swapping algorithms match the way your programs use virtual memory. Fortunately, memory reads and writes that are close together in time also tend to cluster in memory space. This tendency is called locality, or more formally locality of reference — and it's a good thing. If memory references jumped around virtual space at random, you'd typically have to do a disk read and write for each new reference and virtual memory would be as slow as a disk. But because programs do actually exhibit strong locality, your operating system can do relatively few swaps per reference.</para>
			
		<para>It's been found by experience that the most effective method for a broad class of memory-usage patterns is very simple; it's called LRU or the "least recently used" algorithm. The virtual-memory system grabs disk blocks into its working set as it needs them. When it runs out of physical memory for the working set, it dumps the least-recently-used block. All Unixes, and most other virtual-memory operating systems, use minor variations on LRU.</para>
			
		<para>Virtual memory is the first link in the bridge between disk and processor speeds. It's explicitly managed by the OS. But there is still a major gap between the speed of physical main memory and the speed at which a processor can access its register memory. The external and internal caches address this, using a technique similar to virtual memory as I've described it.</para>
			
		<para>Just as the physical main memory behaves like a set of windows or caches on the disk's swap area, the external cache acts as windows on main memory. External cache is faster (250M accesses per sec, rather than 100M) and smaller. The hardware (specifically, your computer's memory controller) does the LRU thing in the external cache on blocks of data fetched from the main memory. For historical reasons, the unit of cache swapping is called a line rather than a page.</para>
			
		<para>But we're not done. The internal cache gives us the final step-up in effective speed by caching portions of the external cache. It is faster and smaller yet — in fact, it lives right on the processor chip.</para>
			
		<para>If you want to make your programs really fast, it's useful to know these details. Your programs get faster when they have stronger locality, because that makes the caching work better. The easiest way to make programs fast is therefore to make them small. If a program isn't slowed down by lots of disk I/O or waits on network events, it will usually run at the speed of the smallest cache that it will fit inside.</para>
			
		<para>If you can't make your whole program small, some effort to tune the speed-critical portions so they have stronger locality can pay off. Details on techniques for doing such tuning are beyond the scope of this tutorial; by the time you need them, you'll be intimate enough with some compiler to figure out many of them yourself.</para>
	</sect2>
	<sect2 id="fundementals-howto-memory-management-mmu"><title>The Memory Management Unit</title>
		<para>Even when you have enough physical core to avoid swapping, the part of the operating system called the memory manager still has important work to do. It has to make sure that programs can only alter their own data segments — that is, prevent erroneous or malicious code in one program from garbaging the data in another. To do this, it keeps a table of data and code segments. The table is updated whenever a process either requests more memory or releases memory (the latter usually when it exits).</para>
			
		<para>This table is used to pass commands to a specialized part of the underlying hardware called an MMU or memory management unit. Modern processor chips have MMUs built right onto them. The MMU has the special ability to put fences around areas of memory, so an out-of-bound reference will be refused and cause a special interrupt to be raised.</para>
			
		<para>If you ever see a Unix message that says "Segmentation fault", "core dumped" or something similar, this is exactly what has happened; an attempt by the running program to access memory (core) outside its segment has raised a fatal interrupt. This indicates a bug in the program code; the core dump it leaves behind is diagnostic information intended to help a programmer track it down.</para>
			
		<para>There is another aspect to protecting processes from each other besides segregating the memory they access. You also want to be able to control their file accesses so a buggy or malicious program can't corrupt critical pieces of the system. This is why Unix has file permissions which we'll discuss later.</para>
	</sect2>
</sect1>

<sect1 id="fundementals-howto-store-in-memory"><title>How does my computer store things in memory?</title>
	<para>You probably know that everything on a computer is stored as strings of bits (binary digits; you can think of them as lots of little on-off switches). Here we'll explain how those bits are used to represent the letters and numbers that your computer is crunching.</para>
		
	<para>Before we can go into this, you need to understand about the word size of your computer. The word size is the computer's preferred size for moving units of information around; technically it's the width of your processor's registers, which are the holding areas your processor uses to do arithmetic and logical calculations. When people write about computers having bit sizes (calling them, say, "32-bit" or "64-bit" computers), this is what they mean.</para>
		
	<para>Most computers (including 386, 486, and Pentium PCs) have a word size of 32 bits. The old 286 machines had a word size of 16. Old-style mainframes often had 36-bit words. The AMD Opteron, Intel Itanium, and the Alpha from what used to be DEC and is now Compaq have 64-bit words.</para>
		
	<para>The computer views your memory as a sequence of words numbered from zero up to some large value dependent on your memory size. That value is limited by your word size, which is why programs on older machines like 286s had to go through painful contortions to address large amounts of memory. I won't describe them here; they still give older programmers nightmares.</para>
	<sect2 id="fundementals-howto-store-in-memory-numbers"><title>Numbers</title>
		<para>Integer numbers are represented as either words or pairs of words, depending on your processor's word size. One 32-bit machine word is the most common integer representation.</para>
			
		<para>Integer arithmetic is close to but not actually mathematical base-two. The low-order bit is 1, next 2, then 4 and so forth as in pure binary. But signed numbers are represented in twos-complement notation. The highest-order bit is a sign bit which makes the quantity negative, and every negative number can be obtained from the corresponding positive value by inverting all the bits and adding one. This is why integers on a 32-bit machine have the range -231 to 231 - 1. That 32nd bit is being used for sign; 0 means a positive number or zero, 1 a negative number.</para>
			
		<para>Some computer languages give you access to unsigned arithmetic which is straight base 2 with zero and positive numbers only.</para>
			
		<para>Most processors and some languages can do operations in floating-point numbers (this capability is built into all recent processor chips). Floating-point numbers give you a much wider range of values than integers and let you express fractions. The ways in which this is done vary and are rather too complicated to discuss in detail here, but the general idea is much like so-called ‘scientific notation’, where one might write (say) 1.234 * 1023; the encoding of the number is split into a mantissa (1.234) and the exponent part (23) for the power-of-ten multiplier (which means the number multiplied out would have 20 zeros on it, 23 minus the three decimal places).</para>
	</sect2>
	<sect2 id="fundementals-howto-store-in-memory-characters"><title>Characters</title>
		<para>Characters are normally represented as strings of seven bits each in an encoding called ASCII (American Standard Code for Information Interchange). On modern machines, each of the 128 ASCII characters is the low seven bits of an octet or 8-bit byte; octets are packed into memory words so that (for example) a six-character string only takes up two memory words. For an ASCII code chart, type ‘man 7 ascii’ at your Unix prompt.</para>
			
		<para>The preceding paragraph was misleading in two ways. The minor one is that the term ‘octet’ is formally correct but seldom actually used; most people refer to an octet as byte and expect bytes to be eight bits long. Strictly speaking, the term ‘byte’ is more general; there used to be, for example, 36-bit machines with 9-bit bytes (though there probably never will be again).</para>
			
		<para>The major one is that not all the world uses ASCII. In fact, much of the world can't — ASCII, while fine for American English, lacks many accented and other special characters needed by users of other languages. Even British English has trouble with the lack of a pound-currency sign.</para>
			
		<para>There have been several attempts to fix this problem. All use the extra high bit that ASCII doesn't, making it the low half of a 256-character set. The most widely-used of these is the so-called ‘Latin-1’ character set (more formally called ISO 8859-1). This is the default character set for Linux, older versions of HTML, and X. Microsoft Windows uses a mutant version of Latin-1 that adds a bunch of characters such as right and left double quotes in places proper Latin-1 leaves unassigned for historical reasons (for a scathing account of the trouble this causes, see the demoroniser page).</para>
			
		<para>Latin-1 handles western European languages, including English, French, German, Spanish, Italian, Dutch, Norwegian, Swedish, Danish, and Icelandic. However, this isn't good enough either, and as a result there is a whole series of Latin-2 through -9 character sets to handle things like Greek, Arabic, Hebrew, Esperanto, and Serbo-Croatian. For details, see the ISO alphabet soup page.</para>
			
		<para>The ultimate solution is a huge standard called Unicode (and its identical twin ISO/IEC 10646-1:1993). Unicode is identical to Latin-1 in its lowest 256 slots. Above these in 16-bit space it includes Greek, Cyrillic, Armenian, Hebrew, Arabic, Devanagari, Bengali, Gurmukhi, Gujarati, Oriya, Tamil, Telugu, Kannada, Malayalam, Thai, Lao, Georgian, Tibetan, Japanese Kana, the complete set of modern Korean Hangul, and a unified set of Chinese/Japanese/Korean (CJK) ideographs. For details, see the Unicode Home Page. XML and XHTML use this character set.</para>
	</sect2>
</sect1>

<sect1 id="fundementals-howto-store-on-disk"><title>How does my computer store things on disk?</title>
	<para>When you look at a hard disk under Unix, you see a tree of named directories and files. Normally you won't need to look any deeper than that, but it does become useful to know what's going on underneath if you have a disk crash and need to try to salvage files. Unfortunately, there's no good way to describe disk organization from the file level downwards, so I'll have to describe it from the hardware up.</para>

	<sect2 id="fundementals-howto-store-on-disk-low-level"><title>Low-level disk and file system structure</title>
		<para>The surface area of your disk, where it stores data, is divided up something like a dartboard — into circular tracks which are then pie-sliced into sectors. Because tracks near the outer edge have more area than those close to the spindle at the center of the disk, the outer tracks have more sector slices in them than the inner ones. Each sector (or disk block) has the same size, which under modern Unixes is generally 1 binary K (1024 8-bit words). Each disk block has a unique address or disk block number.</para>
			
		<para>Unix divides the disk into disk partitions. Each partition is a continuous span of blocks that's used separately from any other partition, either as a file system or as swap space. The original reasons for partitions had to do with crash recovery in a world of much slower and more error-prone disks; the boundaries between them reduce the fraction of your disk likely to become inaccessible or corrupted by a random bad spot on the disk. Nowadays, it's more important that partitions can be declared read-only (preventing an intruder from modifying critical system files) or shared over a network through various means we won't discuss here. The lowest-numbered partition on a disk is often treated specially, as a boot partition where you can put a kernel to be booted.</para>
			
		<para>Each partition is either swap space (used to implement virtual memory) or a file system used to hold files. Swap-space partitions are just treated as a linear sequence of blocks. File systems, on the other hand, need a way to map file names to sequences of disk blocks. Because files grow, shrink, and change over time, a file's data blocks will not be a linear sequence but may be scattered all over its partition (from wherever the operating system can find a free block when it needs one). This scattering effect is called fragmentation.</para>
	</sect2>
	<sect2 id="fundementals-howto-store-on-disk-file-names"><title>File names and directories</title>
		<para>Within each file system, the mapping from names to blocks is handled through a structure called an i-node. There's a pool of these things near the "bottom" (lowest-numbered blocks) of each file system (the very lowest ones are used for housekeeping and labeling purposes we won't describe here). Each i-node describes one file. File data blocks (including directories) live above the i-nodes (in higher-numbered blocks).</para>
			
		<para>Every i-node contains a list of the disk block numbers in the file it describes. (Actually this is a half-truth, only correct for small files, but the rest of the details aren't important here.) Note that the i-node does not contain the name of the file.</para>
			
		<para>Names of files live in directory structures. A directory structure just maps names to i-node numbers. This is why, in Unix, a file can have multiple true names (or hard links); they're just multiple directory entries that happen to point to the same i-node.</para>
	</sect2>
	<sect2 id="fundementals-howto-store-on-disk-mount-points"><title>Mount points</title>
		<para>In the simplest case, your entire Unix file system lives in just one disk partition. While you'll see this arrangement on some small personal Unix systems, it's unusual. More typical is for it to be spread across several disk partitions, possibly on different physical disks. So, for example, your system may have one small partition where the kernel lives, a slightly larger one where OS utilities live, and a much bigger one where user home directories live.</para>
			
		<para>The only partition you'll have access to immediately after system boot is your root partition, which is (almost always) the one you booted from. It holds the root directory of the file system, the top node from which everything else hangs.</para>
			
		<para>The other partitions in the system have to be attached to this root in order for your entire, multiple-partition file system to be accessible. About midway through the boot process, your Unix will make these non-root partitions accessible. It will mount each one onto a directory on the root partition.</para>
			
		<para>For example, if you have a Unix directory called /usr, it is probably a mount point to a partition that contains many programs installed with your Unix but not required during initial boot.</para>
	</sect2>
	<sect2 id="fundementals-howto-store-on-disk-looked-up"><title>How a file gets looked up</title>
		<para>Now we can look at the file system from the top down. When you open a file (such as, say, /home/esr/WWW/ldp/fundamentals.xml) here is what happens:</para>
			
		<para>Your kernel starts at the root of your Unix file system (in the root partition). It looks for a directory there called ‘home’. Usually ‘home’ is a mount point to a large user partition elsewhere, so it will go there. In the top-level directory structure of that user partition, it will look for a entry called ‘esr’ and extract an i-node number. It will go to that i-node, notice that its associated file data blocks are a directory structure, and look up ‘WWW’. Extracting that i-node, it will go to the corresponding subdirectory and look up ‘ldp’. That will take it to yet another directory i-node. Opening that one, it will find an i-node number for ‘fundamentals.xml’. That i-node is not a directory, but instead holds the list of disk blocks associated with the file.</para>
	</sect2>
	<sect2 id="fundementals-howto-store-on-disk-permissions"><title>File ownership, permissions and security</title>
		<para>To keep programs from accidentally or maliciously stepping on data they shouldn't, Unix has permission features. These were originally designed to support timesharing by protecting multiple users on the same machine from each other, back in the days when Unix ran mainly on expensive shared minicomputers.</para>
			
		<para>In order to understand file permissions, you need to recall the description of users and groups in the section What happens when you log in?. Each file has an owning user and an owning group. These are initially those of the file's creator; they can be changed with the programs chown(1) and chgrp(1).</para>
			
		<para>The basic permissions that can be associated with a file are ‘read’ (permission to read data from it), ‘write’ (permission to modify it) and ‘execute’ (permission to run it as a program). Each file has three sets of permissions; one for its owning user, one for any user in its owning group, and one for everyone else. The ‘privileges’ you get when you log in are just the ability to do read, write, and execute on those files for which the permission bits match your user ID or one of the groups you are in, or files that have been made accessible to the world.</para>
			
		<para>To see how these may interact and how Unix displays them, let's look at some file listings on a hypothetical Unix system. Here's one:<screen>snark:~$ ls -l notes
-rw-r--r--   1 esr      users         2993 Jun 17 11:00 notes</screen></para>
		<para>This is an ordinary data file. The listing tells us that it's owned by the user ‘esr’ and was created with the owning group ‘users’. Probably the machine we're on puts every ordinary user in this group by default; other groups you commonly see on timesharing machines are ‘staff’, ‘admin’, or ‘wheel’ (for obvious reasons, groups are not very important on single-user workstations or PCs). Your Unix may use a different default group, perhaps one named after your user ID.</para>
	
		<para>The string ‘-rw-r--r--’ represents the permission bits for the file. The very first dash is the position for the directory bit; it would show ‘d’ if the file were a directory, or would show ‘l’ if the file were a symbolic link. After that, the first three places are user permissions, the second three group permissions, and the third are permissions for others (often called ‘world’ permissions). On this file, the owning user ‘esr’ may read or write the file, other people in the ‘users’ group may read it, and everybody else in the world may read it. This is a pretty typical set of permissions for an ordinary data file.</para>
	
		<para>Now let's look at a file with very different permissions. This file is GCC, the GNU C compiler.
			<screen>snark:~$ ls -l /usr/bin/gcc
-rwxr-xr-x   3 root     bin         64796 Mar 21 16:41 /usr/bin/gcc</screen></para>
		<para>This file belongs to a user called ‘root’ and a group called ‘bin’; it can be written (modified) only by root, but read or executed by anyone. This is a typical ownership and set of permissions for a pre-installed system command. The ‘bin’ group exists on some Unixes to group together system commands (the name is a historical relic, short for ‘binary’). Your Unix might use a ‘root’ group instead (not quite the same as the ‘root' user!).</para>
	
		<para>The ‘root’ user is the conventional name for numeric user ID 0, a special, privileged account that can override all privileges. Root access is useful but dangerous; a typing mistake while you're logged in as root can clobber critical system files that the same command executed from an ordinary user account could not touch.</para>
	
		<para>Because the root account is so powerful, access to it should be guarded very carefully. Your root password is the single most critical piece of security information on your system, and it is what any crackers and intruders who ever come after you will be trying to get.</para>
	
		<para>About passwords: Don't write them down — and don't pick a passwords that can easily be guessed, like the first name of your girlfriend/boyfriend/spouse. This is an astonishingly common bad practice that helps crackers no end. In general, don't pick any word in the dictionary; there are programs called dictionary crackers that look for likely passwords by running through word lists of common choices. A good technique is to pick a combination consisting of a word, a digit, and another word, such as ‘shark6cider’ or ‘jump3joy’; that will make the search space too large for a dictionary cracker. Don't use these examples, though — crackers might expect that after reading this document and put them in their dictionaries.</para>
	
		<para>Now let's look at a third case:<screen>snark:~$ ls -ld ~
drwxr-xr-x  89 esr      users          9216 Jun 27 11:29 /home2/esr
snark:~$ </screen></para>
		<para>This file is a directory (note the ‘d’ in the first permissions slot). We see that it can be written only by esr, but read and executed by anybody else.</para>
	
		<para>Read permission gives you the ability to list the directory — that is, to see the names of files and directories it contains. Write permission gives you the ability to create and delete files in the directory. If you remember that the directory includes a list of the names of the files and subdirectories it contains, these rules will make sense.</para>
	
		<para>Execute permission on a directory means you can get through the directory to open the files and directories below it. In effect, it gives you permission to access the i-nodes in the directory. A directory with execute completely turned off would be useless.</para>
	
		<para>Occasionally you'll see a directory that is world-executable but not world-readable; this means a random user can get to files and directories beneath it, but only by knowing their exact names (the directory cannot be listed).</para>
	
		<para>It's important to remember that read, write, or execute permission on a directory is independent of the permissions on the files and directories beneath. In particular, write access on a directory means you can create new files or delete existing files there, but does not automatically give you write access to existing files.</para>
	
		<para>Finally, let's look at the permissions of the login program itself.<screen>snark:~$ ls -l /bin/login
-rwsr-xr-x   1 root     bin         20164 Apr 17 12:57 /bin/login</screen></para>
		<para>This has the permissions we'd expect for a system command — except for that ‘s’ where the owner-execute bit ought to be. This is the visible manifestation of a special permission called the ‘set-user-id’ or setuid bit.</para>
	
		<para>The setuid bit is normally attached to programs that need to give ordinary users the privileges of root, but in a controlled way. When it is set on an executable program, you get the privileges of the owner of that program file while the program is running on your behalf, whether or not they match your own.</para>
	
		<para>Like the root account itself, setuid programs are useful but dangerous. Anyone who can subvert or modify a setuid program owned by root can use it to spawn a shell with root privileges. For this reason, opening a file to write it automatically turns off its setuid bit on most Unixes. Many attacks on Unix security try to exploit bugs in setuid programs in order to subvert them. Security-conscious system administrators are therefore extra-careful about these programs and reluctant to install new ones.</para>
	
		<para>There are a couple of important details we glossed over when discussing permissions above; namely, how the owning group and permissions are assigned when a file or directory is first created. The group is an issue because users can be members of multiple groups, but one of them (specified in the user's /etc/passwd entry) is the user's default group and will normally own files created by the user.</para>
	
		<para>The story with initial permission bits is a little more complicated. A program that creates a file will normally specify the permissions it is to start with. But these will be modified by a variable in the user's environment called the umask. The umask specifies which permission bits to turn off when creating a file; the most common value, and the default on most systems, is -------w- or 002, which turns off the world-write bit. See the documentation of the umask command on your shell's manual page for details.</para>
	
		<para>Initial directory group is also a bit complicated. On some Unixes a new directory gets the default group of the creating user (this in the System V convention); on others, it gets the owning group of the parent directory in which it's created (this is the BSD convention). On some modern Unixes, including Linux, the latter behavior can be selected by setting the set-group-ID on the directory (chmod g+s).</para>
	</sect2>
	<sect2 id="fundementals-howto-store-on-disk-go-wrong"><title>How things can go wrong</title>
		<para>Earlier it was hinted that file systems can be fragile things. Now we know that to get to a file you have to hopscotch through what may be an arbitrarily long chain of directory and i-node references. Now suppose your hard disk develops a bad spot?</para>
			
		<para>If you're lucky, it will only trash some file data. If you're unlucky, it could corrupt a directory structure or i-node number and leave an entire subtree of your system hanging in limbo — or, worse, result in a corrupted structure that points multiple ways at the same disk block or i-node. Such corruption can be spread by normal file operations, trashing data that was not in the original bad spot.</para>
			
		<para>Fortunately, this kind of contingency has become quite uncommon as disk hardware has become more reliable. Still, it means that your Unix will want to integrity-check the file system periodically to make sure nothing is amiss. Modern Unixes do a fast integrity check on each partition at boot time, just before mounting it. Every few reboots they'll do a much more thorough check that takes a few minutes longer.</para>
			
		<para>If all of this sounds like Unix is terribly complex and failure-prone, it may be reassuring to know that these boot-time checks typically catch and correct normal problems before they become really disastrous. Other operating systems don't have these facilities, which speeds up booting a bit but can leave you much more seriously screwed when attempting to recover by hand (and that's assuming you have a copy of Norton Utilities or whatever in the first place...).</para>
			
		<para>One of the trends in current Unix designs is journalling file systems. These arrange traffic to the disk so that it's guaranteed to be in a consistent state that can be recovered when the system comes back up. This will speed up the boot-time integrity check a lot.</para>
	</sect2>
</sect1>

<sect1 id="fundementals-howto-computer-languages"><title>How do computer languages work?</title>
	<para>We've already discussed how programs are run. Every program ultimately has to execute as a stream of bytes that are instructions in your computer's machine language. But human beings don't deal with machine language very well; doing so has become a rare, black art even among hackers.</para>
		
	<para>Almost all Unix code except a small amount of direct hardware-interface support in the kernel itself is nowadays written in a high-level language. (The ‘high-level’ in this term is a historical relic meant to distinguish these from ‘low-level’ assembler languages, which are basically thin wrappers around machine code.)</para>
		
	<para>There are several different kinds of high-level languages. In order to talk about these, you'll find it useful to bear in mind that the source code of a program (the human-created, editable version) has to go through some kind of translation into machine code that the machine can actually run.</para>
	<sect2 id="fundementals-howto-computer-languages-compiled"><title>Compiled languages</title>
		<para>The most conventional kind of language is a compiled language. Compiled languages get translated into runnable files of binary machine code by a special program called (logically enough) a compiler. Once the binary has been generated, you can run it directly without looking at the source code again. (Most software is delivered as compiled binaries made from code you don't see.)</para>
			
		<para>Compiled languages tend to give excellent performance and have the most complete access to the OS, but also to be difficult to program in.</para>
			
		<para>C, the language in which Unix itself is written, is by far the most important of these (with its variant C++). FORTRAN is another compiled language still used among engineers and scientists but years older and much more primitive. In the Unix world no other compiled languages are in mainstream use. Outside it, COBOL is very widely used for financial and business software.</para>
			
		<para>There used to be many other compiler languages, but most of them have either gone extinct or are strictly research tools. If you are a new Unix developer using a compiled language, it is overwhelmingly likely to be C or C++.</para>
	</sect2>
	<sect2 id="fundementals-howto-computer-languages-interpreted"><title>Interpreted languages</title>
		<para>An interpreted language depends on an interpreter program that reads the source code and translates it on the fly into computations and system calls. The source has to be re-interpreted (and the interpreter present) each time the code is executed.</para>
			
		<para>Interpreted languages tend to be slower than compiled languages, and often have limited access to the underlying operating system and hardware. On the other hand, they tend to be easier to program and more forgiving of coding errors than compiled languages.</para>
			
		<para>Many Unix utilities, including the shell and bc(1) and sed(1) and awk(1), are effectively small interpreted languages. BASICs are usually interpreted. So is Tcl. Historically, the most important interpretive language has been LISP (a major improvement over most of its successors). Today, Unix shells and the Lisp that lives inside the Emacs editor are probably the most important pure interpreted languages.</para>
	</sect2>
	<sect2 id="fundementals-howto-computer-languages-p-code"><title>P-code languages</title>
		<para>Since 1990 a kind of hybrid language that uses both compilation and interpretation has become increasingly important. P-code languages are like compiled languages in that the source is translated to a compact binary form which is what you actually execute, but that form is not machine code. Instead it's pseudocode (or p-code), which is usually a lot simpler but more powerful than a real machine language. When you run the program, you interpret the p-code.</para>
			
		<para>P-code can run nearly as fast as a compiled binary (p-code interpreters can be made quite simple, small and speedy). But p-code languages can keep the flexibility and power of a good interpreter.</para>
			
		<para>Important p-code languages include Python, Perl, and Java.</para>
	</sect2>
</sect1>

<sect1 id="fundementals-howto-internet"><title>How does the Internet work?</title>
	<para>To help you understand how the Internet works, we'll look at the things that happen when you do a typical Internet operation — pointing a browser at the front page of this document at its home on the Web at the Linux Documentation Project. This document is
		
		<screen><ulink url="http://www.tldp.org/HOWTO/Unix-and-Internet-Fundamentals-HOWTO/index.html"/></screen></para>
		
	<para>which means it lives in the file HOWTO/Unix-and-Internet-Fundamentals-HOWTO/index.html under the World Wide Web export directory of the host www.tldp.org.</para>

	<sect2 id="fundementals-howto-internet-names"><title>Names and locations</title>
		<para>The first thing your browser has to do is to establish a network connection to the machine where the document lives. To do that, it first has to find the network location of the host www.tldp.org (‘host’ is short for ‘host machine’ or ‘network host'; www.tldp.org is a typical hostname). The corresponding location is actually a number called an IP address (we'll explain the ‘IP’ part of this term later).</para>
			
		<para>To do this, your browser queries a program called a name server. The name server may live on your machine, but it's more likely to run on a service machine that yours talks to. When you sign up with an ISP, part of your setup procedure will almost certainly involve telling your Internet software the IP address of a nameserver on the ISP's network.</para>
			
		<para>The name servers on different machines talk to each other, exchanging and keeping up to date all the information needed to resolve hostnames (map them to IP addresses). Your nameserver may query three or four different sites across the network in the process of resolving www.tldp.org, but this usually happens very quickly (as in less than a second). We'll look at how nameservers detail in the next section.</para>
			
		<para>The nameserver will tell your browser that www.tldp.org's IP address is 152.19.254.81; knowing this, your machine will be able to exchange bits with www.tldp.org directly.</para>
	</sect2>
	<sect2 id="fundementals-howto-internet-dns"><title>The Domain Name System</title>
		<para>The whole network of programs and databases that cooperates to translate hostnames to IP addresses is called ‘DNS’ (Domain Name System). When you see references to a ‘DNS server’, that means what we just called a nameserver. Now I'll explain how the overall system works.</para>
			
		<para>Internet hostnames are composed of parts separated by dots. A domain is a collection of machines that share a common name suffix. Domains can live inside other domains. For example, the machine www.tldp.org lives in the .tldp.org subdomain of the .org domain.</para>
			
		<para>Each domain is defined by an authoritative name server that knows the IP addresses of the other machines in the domain. The authoritative (or ‘primary') name server may have backups in case it goes down; if you see references to a secondary name server or (‘secondary DNS') it's talking about one of those. These secondaries typically refresh their information from their primaries every few hours, so a change made to the hostname-to-IP mapping on the primary will automatically be propagated.</para>
			
		<para>Now here's the important part. The nameservers for a domain do not have to know the locations of all the machines in other domains (including their own subdomains); they only have to know the location of the nameservers. In our example, the authoritative name server for the .org domain knows the IP address of the nameserver for .tldp.org but not the address of all the other machines in .tldp.org.</para>
			
		<para>The domains in the DNS system are arranged like a big inverted tree. At the top are the root servers. Everybody knows the IP addresses of the root servers; they're wired into your DNS software. The root servers know the IP addresses of the nameservers for the top-level domains like .com and .org, but not the addresses of machines inside those domains. Each top-level domain server knows where the nameservers for the domains directly beneath it are, and so forth.</para>
			
		<para>DNS is carefully designed so that each machine can get away with the minimum amount of knowledge it needs to have about the shape of the tree, and local changes to subtrees can be made simply by changing one authoritative server's database of name-to-IP-address mappings.</para>
			
		<para>When you query for the IP address of www.tldp.org, what actually happens is this: First, your nameserver asks a root server to tell it where it can find a nameserver for .org. Once it knows that, it then asks the .org server to tell it the IP address of a .tldp.org nameserver. Once it has that, it asks the .tldp.org nameserver to tell it the address of the host www.tldp.org.</para>
			
		<para>Most of the time, your nameserver doesn't actually have to work that hard. Nameservers do a lot of cacheing; when yours resolves a hostname, it keeps the association with the resulting IP address around in memory for a while. This is why, when you surf to a new website, you'll usually only see a message from your browser about "Looking up" the host for the first page you fetch. Eventually the name-to-address mapping expires and your DNS has to re-query — this is important so you don't have invalid information hanging around forever when a hostname changes addresses. Your cached IP address for a site is also thrown out if the host is unreachable. </para>
	</sect2>
	<sect2 id="fundementals-howto-internet-packets"><title>Packets and routers</title>
		<para>What the browser wants to do is send a command to the Web server on www.tldp.org that looks like this:
			
			<screen>GET /LDP/HOWTO/Fundamentals.html HTTP/1.0</screen></para>
			
		<para>Here's how that happens. The command is made into a packet, a block of bits like a telegram that is wrapped with three important things; the source address (the IP address of your machine), the destination address (152.19.254.81), and a service number or port number (80, in this case) that indicates that it's a World Wide Web request.</para>
			
		<para>Your machine then ships the packet down the wire (your connection to your ISP, or local network) until it gets to a specialized machine called a router. The router has a map of the Internet in its memory — not always a complete one, but one that completely describes your network neighborhood and knows how to get to the routers for other neighborhoods on the Internet.</para>
			
		<para>Your packet may pass through several routers on the way to its destination. Routers are smart. They watch how long it takes for other routers to acknowledge having received a packet. They also use that information to direct traffic over fast links. They use it to notice when another router (or a cable) have dropped off the network, and compensate if possible by finding another route.</para>
			
		<para>There's an urban legend that the Internet was designed to survive nuclear war. This is not true, but the Internet's design is extremely good at getting reliable performance out of flaky hardware in an uncertain world. This is directly due to the fact that its intelligence is distributed through thousands of routers rather than concentrated in a few massive and vulnerable switches (like the phone network). This means that failures tend to be well localized and the network can route around them.</para>
			
		<para>Once your packet gets to its destination machine, that machine uses the service number to feed the packet to the web server. The web server can tell where to reply to by looking at the command packet's source IP address. When the web server returns this document, it will be broken up into a number of packets. The size of the packets will vary according to the transmission media in the network and the type of service.</para>
	</sect2>
	<sect2 id="fundementals-howto-internet-tcp-ip"><title>TCP and IP</title>
		<para>To understand how multiple-packet transmissions are handled, you need to know that the Internet actually uses two protocols, stacked one on top of the other.</para>
			
		<para>The lower level, IP (Internet Protocol), is responsible for labeling individual packets with the source address and destination address of two computers exchanging information over a network. For example, when you access http://www.tldp.org, the packets you send will have your computer's IP address, such as 192.168.1.101, and the IP address of the www.tldp.org computer, 152.2.210.81. These addresses work in much the same way that your home address works when someone sends you a letter. The post office can read the address and determine where you are and how best to route the letter to you, much like a router does for Internet traffic.</para>
			
		<para>The upper level, TCP (Transmission Control Protocol), gives you reliability. When two machines negotiate a TCP connection (which they do using IP), the receiver knows to send acknowledgements of the packets it sees back to the sender. If the sender doesn't see an acknowledgement for a packet within some timeout period, it resends that packet. Furthermore, the sender gives each TCP packet a sequence number, which the receiver can use to reassemble packets in case they show up out of order. (This can easily happen if network links go up or down during a connection.)</para>
			
		<para>TCP/IP packets also contain a checksum to enable detection of data corrupted by bad links. (The checksum is computed from the rest of the packet in such a way that if either the rest of the packet or the checksum is corrupted, redoing the computation and comparing is very likely to indicate an error.) So, from the point of view of anyone using TCP/IP and nameservers, it looks like a reliable way to pass streams of bytes between hostname/service-number pairs. People who write network protocols almost never have to think about all the packetizing, packet reassembly, error checking, checksumming, and retransmission that goes on below that level.</para>
	</sect2>
	<sect2 id="fundementals-howto-internet-http"><title>HTTP, an application protocol</title>
		<para>Now let's get back to our example. Web browsers and servers speak an application protocol that runs on top of TCP/IP, using it simply as a way to pass strings of bytes back and forth. This protocol is called HTTP (Hyper-Text Transfer Protocol) and we've already seen one command in it — the GET shown above.</para>
			
		<para>When the GET command goes to www.tldp.org's webserver with service number 80, it will be dispatched to a server daemon listening on port 80. Most Internet services are implemented by server daemons that do nothing but wait on ports, watching for and executing incoming commands.</para>
			
		<para>If the design of the Internet has one overall rule, it's that all the parts should be as simple and human-accessible as possible. HTTP, and its relatives (like the Simple Mail Transfer Protocol, SMTP, that is used to move electronic mail between hosts) tend to use simple printable-text commands that end with a carriage-return/line feed.</para>
			
		<para>This is marginally inefficient; in some circumstances you could get more speed by using a tightly-coded binary protocol. But experience has shown that the benefits of having commands be easy for human beings to describe and understand outweigh any marginal gain in efficiency that you might get at the cost of making things tricky and opaque.</para>
			
		<para>Therefore, what the server daemon ships back to you via TCP/IP is also text. The beginning of the response will look something like this (a few headers have been suppressed):
			
			<screen>HTTP/1.1 200 OK
Date: Sat, 10 Oct 1998 18:43:35 GMT
Server: Apache/1.2.6 Red Hat
Last-Modified: Thu, 27 Aug 1998 17:55:15 GMT
Content-Length: 2982
Content-Type: text/html</screen></para>
			
		<para>These headers will be followed by a blank line and the text of the web page (after which the connection is dropped). Your browser just displays that page. The headers tell it how (in particular, the Content-Type header tells it the returned data is really HTML).</para>
	</sect2>
</sect1>

<sect1 id="fundementals-howto-more"><title>To Learn More</title>
	<para>There is a Reading List HOWTO that lists books you can read to learn more about the topics we have touched on here. You might also want to read the How To Become A Hacker document.</para>
</sect1>
</article>